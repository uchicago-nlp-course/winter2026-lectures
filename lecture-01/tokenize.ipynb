{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Tokenization in NLP\n",
    "\n",
    "Breaking text into meaningful units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## What is Tokenization?\n",
    "\n",
    "**Tokenization** is the process of breaking text into smaller units called **tokens**.\n",
    "\n",
    "We will get a quick overview of the following. The goal is to get you to appreciate the complexity of language.\n",
    "- Word tokenization\n",
    "- Character tokenization\n",
    "- Byte-based tokenization\n",
    "- BPE (Byte Pair Encoding) tokenization\n",
    "- Sentence tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Sample Text\n",
    "\n",
    "Let's define a sample text to work with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Communication & Intelligence is awesome!\n"
     ]
    }
   ],
   "source": [
    "text = \"Communication & Intelligence is awesome!\"\n",
    "\n",
    "print(\"Text:\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Word Tokenization\n",
    "\n",
    "Breaking text into **words** or **word-like units**\n",
    "\n",
    "But what are words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Rule-based: Simple .split()\n",
    "\n",
    "The simplest approach: split on whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Communication & Intelligence is awesome!\n",
      "\n",
      "Simple .split() tokens (5):\n",
      "['Communication', '&', 'Intelligence', 'is', 'awesome!']\n"
     ]
    }
   ],
   "source": [
    "# Simple whitespace split\n",
    "simple_tokens = text.split()\n",
    "\n",
    "print(f\"Text: {text}\\n\")\n",
    "print(f\"Simple .split() tokens ({len(simple_tokens)}):\")\n",
    "print(simple_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "**Problem:** Punctuation stays attached to words!\n",
    "\n",
    "- `\"awesome!\"` should probably be `[\"awesome\", \"!\"]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Rule-based: Regular Expressions\n",
    "\n",
    "We can use regex to split on non-word characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Communication & Intelligence is awesome!\n",
      "\n",
      "Regex r'\\w+' tokens (4):\n",
      "['Communication', 'Intelligence', 'is', 'awesome']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Split on word boundaries, keeping only word characters\n",
    "regex_tokens = re.findall(r'\\w+', text)\n",
    "\n",
    "print(f\"Text: {text}\\n\")\n",
    "print(f\"Regex r'\\\\w+' tokens ({len(regex_tokens)}):\")\n",
    "print(regex_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### NLTK Word Tokenizer\n",
    "\n",
    "NLTK provides a more sophisticated tokenizer that handles punctuation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Communication & Intelligence is awesome!\n",
      "\n",
      "NLTK word_tokenize() tokens (6):\n",
      "['Communication', '&', 'Intelligence', 'is', 'awesome', '!']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "nltk_tokens = word_tokenize(text)\n",
    "\n",
    "print(f\"Text: {text}\\n\")\n",
    "print(f\"NLTK word_tokenize() tokens ({len(nltk_tokens)}):\")\n",
    "print(nltk_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Notice:\n",
    "- Punctuation is now separated: `\"awesome!\"` becomes `['awesome', '!']`\n",
    "- `\"&\"` is kept as its own token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Other remaining issues\n",
    "\n",
    "- Lowercase/uppercase? (Communication vs communication)\n",
    "- Contractions? (don't â†’ do n't? don t? dont?)\n",
    "- Hyphenated words? (state-of-the-art â†’ 1 token or 5?)\n",
    "- Possessives? (John's â†’ John 's? Johns?)\n",
    "- Numbers/currency? ($29.99, 1,000,000)\n",
    "- URLs/emails? (https://uchicago.edu)\n",
    "- Multi-word expressions? (New York, ice cream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Character Tokenization\n",
    "\n",
    "Breaking text into individual **characters**\n",
    "\n",
    "**Use cases:**\n",
    "- Character-level language models\n",
    "- Handling rare/unknown words\n",
    "- Languages without clear word boundaries (e.g., Chinese)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Communication & Intelligence is awesome!\n",
      "\n",
      "Character tokens (40):\n",
      "['C', 'o', 'm', 'm', 'u', 'n', 'i', 'c', 'a', 't', 'i', 'o', 'n', ' ', '&', ' ', 'I', 'n', 't', 'e', 'l', 'l', 'i', 'g', 'e', 'n', 'c', 'e', ' ', 'i', 's', ' ', 'a', 'w', 'e', 's', 'o', 'm', 'e', '!']\n"
     ]
    }
   ],
   "source": [
    "# Character tokenization - just use list()\n",
    "char_tokens = list(text)\n",
    "\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"\\nCharacter tokens ({len(char_tokens)}):\")\n",
    "print(char_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Character vs Word Vocabulary Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words: 6\n",
      "Unique characters: 16\n",
      "\n",
      "Unique characters: [' ', '!', '&', 'a', 'c', 'e', 'g', 'i', 'l', 'm', 'n', 'o', 's', 't', 'u', 'w']\n"
     ]
    }
   ],
   "source": [
    "# Compare vocabulary sizes\n",
    "all_words = word_tokenize(text.lower())\n",
    "all_chars = list(text.lower())\n",
    "\n",
    "unique_words = set(all_words)\n",
    "unique_chars = set(all_chars)\n",
    "\n",
    "print(f\"Unique words: {len(unique_words)}\")\n",
    "print(f\"Unique characters: {len(unique_chars)}\")\n",
    "print(f\"\\nUnique characters: {sorted(unique_chars)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Byte-Based Tokenization\n",
    "\n",
    "Breaking text into **bytes** (UTF-8 encoding)\n",
    "\n",
    "**Advantages:**\n",
    "- Universal: works for any language\n",
    "- Fixed vocabulary size (256 possible byte values)\n",
    "- Foundation for BPE tokenization used in GPT models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Communication & Intelligence is awesome!\n",
      "\n",
      "Byte tokens (40):\n",
      "[67, 111, 109, 109, 117, 110, 105, 99, 97, 116, 105, 111, 110, 32, 38, 32, 73, 110, 116, 101, 108, 108, 105, 103, 101, 110, 99, 101, 32, 105, 115, 32, 97, 119, 101, 115, 111, 109, 101, 33]\n"
     ]
    }
   ],
   "source": [
    "# Byte tokenization on our sample text\n",
    "byte_tokens = list(text.encode('utf-8'))\n",
    "\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"\\nByte tokens ({len(byte_tokens)}):\")\n",
    "print(byte_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In hexadecimal:\n",
      "['0x43', '0x6f', '0x6d', '0x6d', '0x75', '0x6e', '0x69', '0x63', '0x61', '0x74', '0x69', '0x6f', '0x6e', '0x20', '0x26', '0x20', '0x49', '0x6e', '0x74', '0x65', '0x6c', '0x6c', '0x69', '0x67', '0x65', '0x6e', '0x63', '0x65', '0x20', '0x69', '0x73', '0x20', '0x61', '0x77', '0x65', '0x73', '0x6f', '0x6d', '0x65', '0x21']\n"
     ]
    }
   ],
   "source": [
    "# Each ASCII letter is 1 byte\n",
    "# Show bytes in hex for readability\n",
    "print(\"In hexadecimal:\")\n",
    "print([hex(b) for b in byte_tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Multilingual Text and Emojis\n",
    "\n",
    "UTF-8 uses variable-length encoding:\n",
    "- ASCII (English letters): 1 byte each\n",
    "- Chinese characters: 3 bytes each\n",
    "- Emojis: 4 bytes each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Hello ä¸–ç•Œ ðŸ˜€\n",
      "Total bytes: 17\n",
      "\n",
      "Breakdown:\n",
      "  'Hello ' = 6 bytes (ASCII)\n",
      "  'ä¸–ç•Œ'   = 6 bytes (Chinese, 3 bytes each)\n",
      "  ' ðŸ˜€'   = 5 bytes (space + emoji, 4 bytes)\n"
     ]
    }
   ],
   "source": [
    "# Multilingual example\n",
    "sample = \"Hello ä¸–ç•Œ ðŸ˜€\"\n",
    "\n",
    "byte_tokens = list(sample.encode('utf-8'))\n",
    "print(f\"Text: {sample}\")\n",
    "print(f\"Total bytes: {len(byte_tokens)}\")\n",
    "print(f\"\\nBreakdown:\")\n",
    "print(f\"  'Hello ' = {len('Hello '.encode('utf-8'))} bytes (ASCII)\")\n",
    "print(f\"  'ä¸–ç•Œ'   = {len('ä¸–ç•Œ'.encode('utf-8'))} bytes (Chinese, 3 bytes each)\")\n",
    "print(f\"  ' ðŸ˜€'   = {len(' ðŸ˜€'.encode('utf-8'))} bytes (space + emoji, 4 bytes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Hello ä¸–ç•Œ ðŸ˜€\n",
      "\n",
      "Word tokenization:\n",
      "  3 tokens: ['Hello', 'ä¸–ç•Œ', 'ðŸ˜€']\n",
      "\n",
      "Character tokenization:\n",
      "  10 tokens: ['H', 'e', 'l', 'l', 'o', ' ', 'ä¸–', 'ç•Œ', ' ', 'ðŸ˜€']\n",
      "\n",
      "Byte tokenization:\n",
      "  17 tokens: [72, 101, 108, 108, 111, 32, 228, 184, 150, 231, 149, 140, 32, 240, 159, 152, 128]\n"
     ]
    }
   ],
   "source": [
    "# Compare tokenization methods on multilingual text\n",
    "print(f\"Text: {sample}\\n\")\n",
    "\n",
    "print(\"Word tokenization:\")\n",
    "words = word_tokenize(sample)\n",
    "print(f\"  {len(words)} tokens: {words}\")\n",
    "\n",
    "print(\"\\nCharacter tokenization:\")\n",
    "characters = list(sample)\n",
    "print(f\"  {len(characters)} tokens: {characters}\")\n",
    "\n",
    "print(\"\\nByte tokenization:\")\n",
    "byte_tokens = list(sample.encode('utf-8'))\n",
    "print(f\"  {len(byte_tokens)} tokens: {byte_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 4. BPE (Byte Pair Encoding) Tokenization\n",
    "\n",
    "**Subword tokenization** - a middle ground between word and character level\n",
    "\n",
    "**Key idea:** Start with characters, iteratively merge the most frequent pairs\n",
    "\n",
    "**Used by:** GPT-2, GPT-3, GPT-4, LLaMA, and many modern LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Communication & Intelligence is awesome!\n",
      "\n",
      "BPE token IDs (6):\n",
      "[66511, 612, 22107, 374, 12738, 0]\n",
      "\n",
      "Decoded tokens:\n",
      "  66511 -> 'Communication'\n",
      "  612 -> ' &'\n",
      "  22107 -> ' Intelligence'\n",
      "  374 -> ' is'\n",
      "  12738 -> ' awesome'\n",
      "  0 -> '!'\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# Use GPT-4's tokenizer (cl100k_base encoding)\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "bpe_tokens = enc.encode(text)\n",
    "\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"\\nBPE token IDs ({len(bpe_tokens)}):\")\n",
    "print(bpe_tokens)\n",
    "\n",
    "print(\"\\nDecoded tokens:\")\n",
    "for token_id in bpe_tokens:\n",
    "    print(f\"  {token_id} -> '{enc.decode([token_id])}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Sentence Tokenization\n",
    "\n",
    "Breaking text into **sentences**\n",
    "\n",
    "Hopefully you know what I am going to ask?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: This course will introduce fundamental concepts in natural language processing (NLP). It will cover the basics of enabling computers to understand and generate language, including word embeddings, language modeling, transformers, and an overview of large language models. It will also cover topics on connections with other disciplines such as linguistics and other social sciences.\n"
     ]
    }
   ],
   "source": [
    "text = \"This course will introduce fundamental concepts in natural language processing (NLP). It will cover the basics of enabling computers to understand and generate language, including word embeddings, language modeling, transformers, and an overview of large language models. It will also cover topics on connections with other disciplines such as linguistics and other social sciences.\"\n",
    "\n",
    "print(\"Text:\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Rule-based: spaCy Sentencizer\n",
    "\n",
    "spaCy's `Sentencizer` uses simple punctuation rules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 sentences (spaCy rule-based):\n",
      "\n",
      "1. This course will introduce fundamental concepts in natural language processing (NLP).\n",
      "2. It will cover the basics of enabling computers to understand and generate language, including word embeddings, language modeling, transformers, and an overview of large language models.\n",
      "3. It will also cover topics on connections with other disciplines such as linguistics and other social sciences.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# Create a blank English model with rule-based sentencizer\n",
    "nlp = English()\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "doc = nlp(text)\n",
    "sentences = list(doc.sents)\n",
    "\n",
    "print(f\"Found {len(sentences)} sentences (spaCy rule-based):\\n\")\n",
    "for i, sent in enumerate(sentences, 1):\n",
    "    print(f\"{i}. {sent.text.strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### NLTK Sentence Tokenizer\n",
    "\n",
    "NLTK uses a trained model (Punkt) that handles abbreviations better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 sentences (NLTK Punkt):\n",
      "\n",
      "1. This course will introduce fundamental concepts in natural language processing (NLP).\n",
      "2. It will cover the basics of enabling computers to understand and generate language, including word embeddings, language modeling, transformers, and an overview of large language models.\n",
      "3. It will also cover topics on connections with other disciplines such as linguistics and other social sciences.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk_sentences = sent_tokenize(text)\n",
    "\n",
    "print(f\"Found {len(nltk_sentences)} sentences (NLTK Punkt):\\n\")\n",
    "for i, sent in enumerate(nltk_sentences, 1):\n",
    "    print(f\"{i}. {sent.strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# What about this text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I work at U.of.C. What about you?\n"
     ]
    }
   ],
   "source": [
    "text = \"I work at U.of.C. What about you?\"\n",
    "\n",
    "print(\"Text:\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 sentences (spaCy rule-based):\n",
      "\n",
      "1. I work at U.of.\n",
      "2. C. What about you?\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "sentences = list(doc.sents)\n",
    "\n",
    "print(f\"Found {len(sentences)} sentences (spaCy rule-based):\\n\")\n",
    "for i, sent in enumerate(sentences, 1):\n",
    "    print(f\"{i}. {sent.text.strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 sentences (NLTK Punkt):\n",
      "\n",
      "1. I work at U.of.C.\n",
      "2. What about you?\n"
     ]
    }
   ],
   "source": [
    "nltk_sentences = sent_tokenize(text)\n",
    "\n",
    "print(f\"Found {len(nltk_sentences)} sentences (NLTK Punkt):\\n\")\n",
    "for i, sent in enumerate(nltk_sentences, 1):\n",
    "    print(f\"{i}. {sent.strip()}\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "rise": {
   "header": "<style>\n.reveal,\n.reveal p,\n.reveal li,\n.reveal .slides section {\n    font-family: 'Avenir', 'Avenir Next', Helvetica, sans-serif !important;\n    font-size: 1.5vw !important;\n}\n.reveal h1,.reveal h2,.reveal h3,.reveal h4 {\n    font-family: 'Avenir', 'Avenir Next', Helvetica, sans-serif !important;\n    color: #800000 !important;\n    font-weight: 600 !important;\n}\n.reveal h1 { font-size: 3.5vw !important; }\n.reveal h2 { font-size: 2.5vw !important; }\n.reveal h3 { font-size: 2vw !important; }\n.reveal h4 { font-size: 1.75vw !important; }\n\n/* Force all code elements */\n.reveal code,\n.reveal pre,\n.reveal pre *,\n.reveal code *,\n.reveal .jp-Cell *,\n.reveal .jp-CodeCell *,\n.reveal .jp-InputArea *,\n.reveal .jp-OutputArea *,\n.reveal .jp-RenderedText *,\n.reveal .jp-RenderedText pre,\n.reveal .cm-line,\n.reveal .cm-content,\n.reveal [class*=\"jp-\"] pre,\n.reveal [class*=\"jp-\"] code {\n    font-size: 1vw !important;\n}\n</style>",
   "margin": 0.04,
   "pdfMaxPagesPerSlide": 1,
   "pdfSeparateFragments": false,
   "scroll": true,
   "slideNumber": true,
   "theme": "white",
   "transition": "slide"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
