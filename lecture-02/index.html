<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Lecture 2: Words and Word Representations | CMSC 25700/35100 NLP</title>
    <meta name="description" content="Words and Word Representations in Natural Language Processing. Topics include tokenization, types/tokens, Zipf's law, morphology, BPE, and word embeddings.">
    <meta name="author" content="Chenhao Tan">
    <meta name="keywords" content="NLP, Natural Language Processing, Tokenization, Word Embeddings, BPE, PMI, Cosine Similarity, University of Chicago">
    <meta property="og:title" content="Lecture 2: Words and Word Representations | CMSC 25700/35100 NLP">
    <meta property="og:description" content="Words and Word Representations in Natural Language Processing at University of Chicago.">
    <meta property="og:type" content="website">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.5.0/reveal.min.css">
    <link rel="stylesheet" href="../assets/css/uchicago-theme.css">
</head>
<body>
    <div class="reveal">
        <div class="slides">

            <section data-markdown>
                <textarea data-template>
                    # CMSC 25700/35100:
                    ## Natural Language Processing

                    ## Lecture 2: Words and Word Representations

                    <p style="text-align: center;">
                    <strong>Chenhao Tan</strong><br/>
                    University of Chicago<br/>
                    @ChenhaoTan, chenhao@uchicago.edu
                    </p>
                    <a
                    href="https://github.com/uchicago-nlp-course/winter-2025-students/issues/1">Submit
                    Music Recommendations</a>
                </textarea>
            </section>



            <!-- Tokenization Section -->
            <section>
                <section data-markdown>
                    <textarea data-template>
                        ## Review of Lecture 1
                        - Word tokenization
                        - Sentence tokenization
                    </textarea>
                </section>

                <section data-markdown>
                    <textarea data-template>
                        ## Counting is the superpower after tokenization.
                    </textarea>
                </section>

                <section data-markdown>
                    <textarea data-template>
                        ## Zipf's Law

                        Frequency of a word is inversely proportional to its rank in the word frequency list

                        $$\text{word frequency} \propto \frac{1}{\text{rank}}$$

                        - "the" is rank 1, most frequent
                        - Most words are extremely rare ("long tail")

                        TODO: add plot
                    </textarea>
                </section>

                <section data-markdown>
                    <textarea data-template>
                        ## Type/Token Ratio

                        How does the type/token ratio change when adding more data?

                        **More data → lower type/token ratio** <!-- .element: class="fragment" -->
                    </textarea>
                </section>

                <section data-markdown>
                    <textarea data-template>
                        ## Which has a higher type/token ratio?

                        Wikipedia (The Free Encyclopedia) vs. Wikipedia (Simple English) 

                        English Wikipedia has higher type/token ratio (more diverse vocabulary) <!-- .element: class="fragment" -->
                    </textarea>
                </section>

                <section data-markdown>
                    <textarea data-template>
                        ## Wikipedia vs. Tweets 

                        Tweets have higher type/token ratio at large scale <!-- .element: class="fragment" data-fragment-index="1" -->

                        Why? Creative spellings like: <!-- .element: class="fragment" data-fragment-index="2" -->
                        - really, rly, realy, rlly, reallly, realllly, reallyy... <!-- .element: class="fragment" data-fragment-index="2" -->
                        - (224,571 "really" vs. hundreds of variants) <!-- .element: class="fragment" data-fragment-index="2" -->
                    </textarea>
                </section>

            </section>

            <!-- Morphology Section -->
            <section>
                <section data-markdown>
                    <textarea data-template>
                        ## Morphology in NLP

                        Morphology is the study of word structure and formation (stems, roots, prefixes, suffixes)

                        NLP problems that address morphology:
                        - **lemmatization**
                        - **stemming**
                    </textarea>
                </section>

                <section data-markdown>
                    <textarea data-template>
                        ## Terminology

                        - **lemma**: canonical/dictionary form of a word
                            - Words with same lemma have same stem, part of speech, rough semantics

                        - **word form**: full inflected or derived form of a word as it appears in text

                        | word form | lemma |
                        |-----------|-------|
                        | run | run |
                        | ran | run |
                        | running | run |

                    </textarea>
                </section>

                <section data-markdown>
                    <textarea data-template>
                        ## Lemmatization

                        **lemmatization**: convert wordform to lemma

                        - am, is, are → be
                        - car, cars, car's, cars' → car

                        Example:
                        - "the boy's cars are different colors"
                        - → "the boy car be different color"

                        Mostly about finding the correct dictionary entry, but this may depend on context

                        
                    </textarea>
                </section>

                <section data-markdown>
                    <textarea data-template>
                        ## Stemming

                        **stemming**: reduce words to their stems by removing affixes
                        - Usually implemented with language-specific, manually-designed rules
                        - Commonly used in information retrieval

                        Example:
                        - "Caillou is an average, imaginative four-year-old boy..."
                        - → "Caillou is an **averag** **imagin** four year old **boi**..."

                        
                    </textarea>
                </section>

                <section data-markdown>
                    <textarea data-template>
                        ## Porter's Algorithm

                        (the most common English stemmer)

                        **Step 1a**
                        - sses → ss (caresses → caress)
                        - ies → i (ponies → poni)
                        - ss → ss (caress → caress)
                        - s → ø (cats → cat)

                        **Step 1b**
                        - (\*v\*)ing → ø (walking → walk, sing → sing)
                        - (\*v\*)ed → ø (plastered → plaster)

                        This is a simplified first step, see <a
                        href="https://vijinimallawaarachchi.com/2017/05/09/porter-stemming-algorithm/
                        ">this page</a> for full details.
                    </textarea>
                </section>

                <section data-markdown>
                    <textarea data-template>
                        ## Idiosyncrasies of the Porter Stemmer

                        | Words | Stem |
                        |-------|------|
                        | sever, severed, severing, several, severe, severely, severity | sever |
                        | wit, wits, witness, witnesses, witnessing | wit |

                        Problems with conflating unrelated words!
                    </textarea>
                </section>

                <section data-markdown>
                    <textarea data-template>
                        ## Lemmatization vs. Stemming

                        **Lemmatization**
                        - Viewed as an NLP task
                        - Solved with dictionary look-up, possibly with machine learning

                        **Stemming**
                        - Uses manually-defined rules
                        - Simple and fast, but limited due to reliance on rules
                        - May conflate words erroneously

                        Both may remove information. To mitigate, combine lemma/stem form with
                        original form, use both!
                        
                    </textarea>
                </section>

                <section data-markdown>
                    <textarea data-template>
                        ## Summary

                        Similar to tokenization, morphology is a human concept. 
                        - Academic definitions may differ from practical use.
                        - Human-designed algorithms may not align with linguistic intuitions.
                        - Machines may learn different patterns with sufficient data.

                        Understanding this difference is important for approaching current LLMs!
                    </textarea>
                </section>
            </section>



            <!-- Word Embeddings Section -->
            <section>
                <section data-markdown>
                    <textarea data-template>
                        ## How do we represent words computationally?

                        荃者所以在鱼，得鱼而忘荃 Nets are for fish;
                        Once you get the fish, you can forget the net.

                        言者所以在意，得意而忘言 Words are for meaning;
                        Once you get the meaning, you can forget the words.

                        <p class="citation">庄子 (Zhuangzi), Chapter 5</p>
                    </textarea>
                </section>

                <section data-markdown>
                    <textarea data-template>
                        ## Canonical Example: king - man + woman = queen

                        <img src="king-queen.webp" style="max-width: 80%; display: block; margin: 0 auto;">

                        <p class="citation">[photo credit: Jaron Collis on Quora]</p>
                    </textarea>
                </section>

                <section data-markdown>
                    <textarea data-template>
                        ## Word Representations

                        Simple approach: assign each word an ID
                        - king → 17
                        - queen → 91
                        - man → 253
                        - ...

                        Better approach: **embeddings** (vectors of real numbers)

                    </textarea>
                </section>

                <section data-markdown>
                    <textarea data-template>
                        ## Motivation for Word Embeddings

                        **Variability**: multiple forms, similar meaning

                        really, reallly, realllly → should have similar vectors!

                        Embeddings can capture this similarity.
                    </textarea>
                </section>

                <section data-markdown>
                    <textarea data-template>
                        ## How should we embed words?

                        **Distributional hypothesis**: words that appear in similar contexts have similar meanings

                        > "You shall know a word by the company it keeps."
                        > Firth (1957)

                    </textarea>
                </section>
            </section>

            <!-- Co-occurrence Counts Section -->
            <section>
                <section data-markdown>
                    <textarea data-template>
                        ## Co-occurrence Counts

                        Build a matrix:
                        - Rows: context words (the, king, queen, man, woman, ...)
                        - Columns: words we're computing vectors for

                        Each cell: count of how often words appear together within a window
                    </textarea>
                </section>

                <section data-markdown>
                    <textarea data-template>
                        ## Window Size

                        "... years later , turing received a posthumous royal pardon from **queen**
                        elizabeth ii . today , the “ turing law ..."

                        - Window size w=1: count immediate neighbors
                        - Window size w=4: count words within 4 positions

                        Larger windows capture more topical similarity; smaller windows capture more syntactic similarity.
                    </textarea>
                </section>
            </section>

            <!-- Similarity Measures Section -->
            <section>
                <section data-markdown>
                    <textarea data-template>
                        ## Computing Similarity

                        Once we have word vectors, we can compute similarities!

                        **Dot product** (inner product):

                        $$\mathbf{u} \cdot \mathbf{v} = \mathbf{u}^\top \mathbf{v} = \sum_i u_i v_i$$

                        Dot product is large when vectors have large values in the same dimensions.
                    </textarea>
                </section>

                <section data-markdown>
                    <textarea data-template>
                        ## Nearest Neighbors with Dot Product

                        | king | queen | man | woman |
                        |------|-------|-----|-------|
                        | the | the | , | of |
                        | of | , | the | , |
                        | , | of | of | . |

                        Top neighbors are just the most frequent words!
                    </textarea>
                </section>

                <section data-markdown>
                    <textarea data-template>
                        ## Problem with Dot Product

                        Dot product doesn't control for vector length!

                        **Vector length**: $\|\mathbf{u}\| = \sqrt{\sum_i u_i^2}$

                        **Cosine similarity**:

                        $$\frac{\mathbf{u}^\top \mathbf{v}}{\|\mathbf{u}\| \|\mathbf{v}\|}$$

                        This is the cosine of the angle between the two vectors!
                    </textarea>
                </section>

                <section data-markdown>
                    <textarea data-template>
                        ## Nearest Neighbors with Cosine Similarity

                        | king | queen | man | woman |
                        |------|-------|-----|-------|
                        | governor | king | woman | man |
                        | queen | lord | girl | person |
                        | ruler | chief | student | girl |

                        Much better than dot product!
                    </textarea>
                </section>
            </section>

            <!-- Improving Counts Section -->
            <section>
                <section data-markdown>
                    <textarea data-template>
                        ## Improving Counts

                        - Counts of common words ("the") are very large, but not very useful
                        - Many ways proposed for improving raw counts:
                            - **tf-idf** ("term frequency-inverse document frequency")
                            - **PMI** (pointwise mutual information; we will skip this in lecture)
                    </textarea>
                </section>

                <section data-markdown>
                    <textarea data-template>
                        ## tf-idf

                        - **tf** is just the count of the two words
                        - **idf** is computed for the context word
                            - document frequency = number of documents in which a word appears
                            - inverse of document frequency will be small for common words
                        - Vector entry is then the product of tf and idf
                    </textarea>
                </section>
            </section>

            <!-- SVD Section -->
            <section>
                <section data-markdown>
                    <textarea data-template>
                        ## From Sparse to Dense Vectors

                        Co-occurrence matrices are:
                        - **Sparse**: most entries are zero
                        - **High-dimensional**: vocabulary size can be 50K+ dimensions

                        Can we get **dense, low-dimensional** representations?
                    </textarea>
                </section>

                <section data-markdown>
                    <textarea data-template>
                        ## Singular Value Decomposition (SVD)

                        Factor a matrix $\mathbf{M}$ into three matrices:

                        $$\mathbf{M} = \mathbf{U} \mathbf{\Sigma} \mathbf{V}^\top$$

                        - $\mathbf{M}$: original matrix (words $\times$ contexts)
                        - $\mathbf{U}$: left singular vectors (words $\times$ k)
                        - $\mathbf{\Sigma}$: diagonal matrix of singular values (k $\times$ k)
                        - $\mathbf{V}^\top$: right singular vectors (k $\times$ contexts)
                    </textarea>
                </section>

                <section data-markdown>
                    <textarea data-template>
                        ## Truncated SVD

                        Keep only the top $k$ singular values:

                        $$\mathbf{M} \approx \mathbf{U}_k \mathbf{\Sigma}_k \mathbf{V}_k^\top$$

                        - Reduces dimensionality from |V| to k (typically 100-300)
                        - Best rank-k approximation (minimizes reconstruction error)
                        - Word vectors: rows of $\mathbf{U}_k$ (or $\mathbf{U}_k \mathbf{\Sigma}_k$)
                    </textarea>
                </section>

                <section data-markdown>
                    <textarea data-template>
                        ## Latent Semantic Analysis (LSA)

                        Apply SVD to document-term matrix:
                        - Rows: documents
                        - Columns: words
                        - Entries: tf-idf weights

                    </textarea>
                </section>

                <section data-markdown>
                    <textarea data-template>
                        ## Why Does SVD Help?

                        - Captures latent semantic structure
                        - Smooths over sparsity in the original counts
                        - Provides continuous representations
                    </textarea>
                </section>


            </section>

            <!-- Word2Vec Section -->
            <section>
                <section data-markdown>
                    <textarea data-template>
                        ## Word2Vec

                        <img src="word2vec.png" style="max-width: 60%; display: block; margin: 0 auto;">

                        `The main goal of this paper is to introduce techniques that can be used
                        for learning high-quality word vectors from **huge** data sets with billions of
                        words, and with millions of words in the vocabulary.`
                    </textarea>
                </section>

                <section data-markdown>
                    <textarea data-template>
                        ## Word2Vec: Main Idea

                        Key insight: Instead of counting co-occurrences, **predict** them

                        Predict context words within a window using a center word

                        <img src="skip_gram.png" style="max-width: 60%; display: block; margin: 0 auto;">
                    </textarea>
                </section>


                <section data-markdown>
                    <textarea data-template>
                        ## Word2Vec: Training Procedure

                        - Start with a large corpus of text
                        - Go through each position $t$ in the text
                        - Predict context words using center word vectors
                            - Compute $P(o|c)$ using similarity of word vectors
                        - Adjust vectors via SGD to minimize the following objective function

                        $$J(\theta) = -\frac{1}{T} \log L(\theta) = -\frac{1}{T} \sum{t=1}^{T} \sum_{-m \leq j \leq m, j \neq 0} P(w_{t+j} | w_t; \theta)$$
                    </textarea>
                </section>

                
                <section data-markdown>
                    <textarea data-template>
                        ## Skip-gram: How to Compute P(context|center)?

                        Use two vectors per word $w$:
                        - $v_w$: vector when $w$ is center word
                        - $u_w$: vector when $w$ is context word

                        $$P(o|c) = \frac{\exp(u_o^\top v_c)}{\sum_{w \in V} \exp(u_w^\top v_c)}$$

                        This is a **softmax** over the vocabulary!
                    </textarea>
                </section>

                <section data-markdown>
                    <textarea data-template>
                        ## The Problem with Softmax

                        $$P(o|c) = \frac{\exp(u_o^\top v_c)}{\sum_{w \in V} \exp(u_w^\top v_c)}$$

                        Denominator sums over **entire vocabulary**!
                        - Vocabulary size |V| can be 100K+ words
                        - Too expensive to compute for every training step
                    </textarea>
                </section>

                <section data-markdown>
                    <textarea data-template>
                        ## Negative Sampling

                        Instead of normalizing over all words, sample **negative examples** and
                        reformulate the problem as binary classification.

                        New objective: distinguish real context words from random words

                        $$\log P(o|c) = \log \sigma(u_o^\top v_c) + \sum_{k \in K} \log \sigma(-u_{w_k}^\top v_c)$$

                        - $\sigma(x) = \frac{1}{1+e^{-x}}$ (sigmoid function)
                        - Sample K negative words (typically K = 5-20)
                    </textarea>
                </section>

                <section data-markdown>
                    <textarea data-template>
                        ## Training with SGD

                        **Stochastic Gradient Descent**:
                        1. Sample a center word and context window
                        2. Compute gradients of objective
                        3. Update word vectors: $\theta \leftarrow \theta - \alpha \nabla_\theta J$

                        After training:
                        - Can use $v_w$ vectors
                        - Or average $\frac{v_w + u_w}{2}$
                    </textarea>
                </section>

                <section>
                    <h2>Two Variants</h2>
                    <div style="display: flex; align-items: center;">
                        <div style="flex: 1; text-align: left;">
                            <p><strong>Skip-gram</strong>: Predict context words from center word</p>
                            <ul>
                                <li>Input: center word</li>
                                <li>Output: context words</li>
                            </ul>
                            <p><strong>CBOW (Continuous Bag of Words)</strong>: Predict center from context</p>
                            <ul>
                                <li>Input: context words</li>
                                <li>Output: center word</li>
                            </ul>
                        </div>
                        <div style="flex: 1;">
                            <img src="cbow_skip_gram.png" style="max-width: 100%;">
                        </div>
                    </div>
                </section>

                <section data-markdown>
                    <textarea data-template>
                        ## Evaluating Word Vectors

                        **Intrinsic evaluation**:
                        - Word analogy task
                        - Word similarity correlation
                        - Fast, but may not reflect downstream performance

                        **Extrinsic evaluation**:
                        - Use vectors in real NLP task (sentiment classification, information extraction, etc.)
                        - Slower, but measures actual usefulness
                    </textarea>
                </section>

                <section data-markdown>
                    <textarea data-template>
                        ## Word Analogies

                        Word vectors capture semantic relationships!

                        **man : woman :: king : ?**

                        $$v_{king} - v_{man} + v_{woman} \approx v_{queen}$$

                        Find word whose vector is closest to $v_{king} - v_{man} + v_{woman}$
                    </textarea>
                </section>

                <section data-markdown>
                    <textarea data-template>
                        ## Types of Analogies

                        | Relationship | Example |
                        |--------------|---------|
                        | Gender | king:queen :: man:woman |
                        | Verb tense | walking:walked :: swimming:swam |
                        | Country-capital | France:Paris :: Italy:Rome |
                        | Comparative | big:bigger :: slow:slower |
                    </textarea>
                </section>

                <section data-markdown>
                    <textarea data-template>
                        ## A Small Replication on Analogy Task
                        <img src="analogy_eval.png" style="max-width: 100%;">
                        
                        See the notebook/html in the lecture folder for details!
                    </textarea>
                </section>

                <section data-markdown>
                    <textarea data-template>
                        ## Scaling law for word embeddings
                        <img src="scaling_law.png" style="max-width: 100%;">
                        
                    </textarea>
                </section>

            </section>
            
            <!-- Conclusion Section -->
            <section>
                <section data-markdown>
                    <textarea data-template>
                        ## Summary

                        - **Distributional hypothesis**: similar contexts = similar meanings
                        - **Co-occurrence matrices**: count-based representations
                        - **SVD/LSA**: dimensionality reduction for dense vectors
                        - **Word2Vec**: predict context words, learn vectors via SGD
                        - SVD and Word2Vec are actually closely related!
                        <p class="citation"><a href="https://papers.nips.cc/paper_files/paper/2014/hash/b78666971ceae55a8e87efb7cbfd9ad4-Abstract.html">Levy & Goldberg (2014)</a></p>
                    </textarea>
                </section>
            
                <section data-markdown>
                    <textarea data-template>
                        ## Word Meaning is not Stable

                        Problem: Words have multiple meanings!

                        - "bank" (financial) vs "bank" (river)
                        - "cell" (biology) vs "cell" (phone)
                        - "this is sick" (ill) vs "this is sick" (cool)

                        Standard word vectors: one vector per word type
                        - Averages over all senses

                        Solutions: contextualized embeddings (later!)
                    </textarea>
                </section>

                <section data-markdown>
                    <textarea data-template>
                        # Questions?
                    </textarea>
                </section>
            </section>
        </div>
    </div>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.5.0/reveal.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.5.0/plugin/markdown/markdown.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.5.0/plugin/highlight/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.5.0/plugin/math/math.min.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.5.0/plugin/highlight/monokai.min.css">
    <script>
        Reveal.initialize({
            hash: true,
            slideNumber: true,
            transition: 'slide',
            width: 1280,
            height: 720,
            margin: 0.04,
            plugins: [ RevealMarkdown, RevealHighlight, RevealMath.KaTeX ]
        });
    </script>
</body>
</html>
