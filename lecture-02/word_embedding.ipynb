{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Lecture 2: Words and Word Representations\n",
    "## Demo Notebook\n",
    "\n",
    "This notebook accompanies Lecture 2 and demonstrates:\n",
    "1. Co-occurrence counts and word-context matrices\n",
    "2. Similarity measures (dot product, cosine similarity)\n",
    "3. Nearest neighbors (count-based, SVD-based, LSA-based)\n",
    "4. Word2Vec embeddings\n",
    "5. Analogy evaluation across all methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport pandas as pd\nfrom collections import Counter, defaultdict\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\nfrom datasets import load_dataset\nfrom nltk.tokenize import word_tokenize\nimport nltk\nnltk.download('punkt_tab', quiet=True)\n\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nnp.random.seed(42)"
  },
  {
   "cell_type": "markdown",
   "id": "data-loading",
   "metadata": {},
   "source": [
    "## Loading Simple Wikipedia\n",
    "\n",
    "We use the [Simple Wikipedia](https://huggingface.co/datasets/rahular/simple-wikipedia) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Simple Wikipedia dataset (first 100k documents)\n",
    "print(\"Loading Simple Wikipedia dataset...\")\n",
    "simplewiki_dataset = load_dataset(\"rahular/simple-wikipedia\", split=\"train[:100000]\")\n",
    "print(f\"Dataset loaded: {len(simplewiki_dataset)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tokenize-corpus",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(data, max_docs=None):\n",
    "    \"\"\"Loads simple wikipedia and returns tokenized documents.\"\"\"\n",
    "    raw_texts = data[\"text\"][:max_docs] if max_docs else data[\"text\"]\n",
    "    tokenized_docs = [[w.lower() for w in word_tokenize(doc)] for doc in tqdm(raw_texts)]\n",
    "    return tokenized_docs\n",
    "\n",
    "print(\"Tokenizing corpus...\")\n",
    "tokenized = read_corpus(simplewiki_dataset)\n",
    "print(f\"Tokenized {len(tokenized)} documents\")\n",
    "print(f\"Sample: {tokenized[1][:15]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build-vocab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary\n",
    "all_words = [w for doc in tokenized for w in doc]\n",
    "word_counts = Counter(all_words)\n",
    "\n",
    "# Filter to words with count >= 5\n",
    "min_count = 5\n",
    "vocab = sorted([w for w, c in word_counts.items() if c >= min_count])\n",
    "word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "\n",
    "print(f\"Total tokens: {len(all_words)}\")\n",
    "print(f\"Vocabulary size (min_count={min_count}): {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooccurrence-intro",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Co-occurrence Counts\n",
    "\n",
    "Build a word-context matrix where each cell counts how often words appear together within a window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooccurrence-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cooccurrence_matrix(tokenized_corpus, word2idx, window_size=2):\n",
    "    \"\"\"Build a word-context co-occurrence matrix.\"\"\"\n",
    "    V = len(word2idx)\n",
    "    cooccur = np.zeros((V, V), dtype=np.float32)\n",
    "    \n",
    "    for doc in tqdm(tokenized_corpus):\n",
    "        for i, word in enumerate(doc):\n",
    "            if word not in word2idx:\n",
    "                continue\n",
    "            word_idx = word2idx[word]\n",
    "            start = max(0, i - window_size)\n",
    "            end = min(len(doc), i + window_size + 1)\n",
    "            \n",
    "            for j in range(start, end):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                context_word = doc[j]\n",
    "                if context_word in word2idx:\n",
    "                    context_idx = word2idx[context_word]\n",
    "                    cooccur[word_idx, context_idx] += 1\n",
    "    \n",
    "    return cooccur\n",
    "\n",
    "print(\"Building co-occurrence matrix...\")\n",
    "cooccur_matrix = build_cooccurrence_matrix(tokenized, word2idx, window_size=2)\n",
    "print(f\"Co-occurrence matrix shape: {cooccur_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooccur-cosine",
   "metadata": {},
   "outputs": [],
   "source": "# Compute pairwise cosine similarity for co-occurrence vectors\nprint(\"Computing pairwise cosine similarity...\")\ncooccur_cosine_sim = cosine_similarity(cooccur_matrix)\nprint(f\"Similarity matrix shape: {cooccur_cosine_sim.shape}\")"
  },
  {
   "cell_type": "markdown",
   "id": "svd-intro",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: SVD on Co-occurrence Matrix\n",
    "\n",
    "Apply Truncated SVD to reduce dimensionality:\n",
    "\n",
    "$$\\mathbf{M} \\approx \\mathbf{U}_k \\mathbf{\\Sigma}_k \\mathbf{V}_k^\\top$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "svd",
   "metadata": {},
   "outputs": [],
   "source": "n_components = 100\nprint(f\"Applying Truncated SVD with {n_components} components...\")\n\nsvd_cooccur = TruncatedSVD(n_components=n_components, n_iter=10, random_state=42)\nsvd_cooccur_vectors = svd_cooccur.fit_transform(cooccur_matrix)\n\nprint(f\"SVD vectors shape: {svd_cooccur_vectors.shape}\")\nprint(f\"Explained variance ratio: {svd_cooccur.explained_variance_ratio_.sum():.2%}\")\n\n# Compute cosine similarity\nsvd_cooccur_cosine_sim = cosine_similarity(svd_cooccur_vectors)"
  },
  {
   "cell_type": "markdown",
   "id": "lsa-intro",
   "metadata": {},
   "source": "---\n# Part 3: LSA (Latent Semantic Analysis)\n\nApply SVD to **document-word matrix** (raw counts):\n- Rows: documents\n- Columns: words\n- Entries: word counts\n\nThis captures document-level co-occurrence patterns rather than local window-based patterns."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build-doc-word",
   "metadata": {},
   "outputs": [],
   "source": "# Build document-word matrix with raw counts\ndef build_doc_word_matrix(tokenized_corpus, word2idx):\n    \"\"\"Build a document-word count matrix.\"\"\"\n    n_docs = len(tokenized_corpus)\n    V = len(word2idx)\n    doc_word = np.zeros((n_docs, V), dtype=np.float32)\n    \n    for doc_idx, doc in enumerate(tqdm(tokenized_corpus)):\n        for word in doc:\n            if word in word2idx:\n                word_idx = word2idx[word]\n                doc_word[doc_idx, word_idx] += 1\n    \n    return doc_word\n\nprint(\"Building document-word matrix...\")\ndoc_word_matrix = build_doc_word_matrix(tokenized, word2idx)\n\nprint(f\"Document-word matrix shape: {doc_word_matrix.shape}\")\nprint(f\"  (documents x words)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lsa-svd",
   "metadata": {},
   "outputs": [],
   "source": "# Apply SVD to get LSA word vectors\n# We need word vectors, so we use V^T from the SVD decomposition\nprint(f\"Applying SVD with {n_components} components for LSA...\")\n\nsvd_lsa = TruncatedSVD(n_components=n_components, n_iter=10, random_state=42)\ndoc_vectors = svd_lsa.fit_transform(doc_word_matrix)  # U * Sigma\n\n# Word vectors are the columns of V^T (or rows of V)\n# svd.components_ gives V^T (n_components x n_features)\n# So we transpose to get word vectors (n_features x n_components)\nlsa_word_vectors = svd_lsa.components_.T  # (vocab_size x n_components)\n\nprint(f\"LSA word vectors shape: {lsa_word_vectors.shape}\")\nprint(f\"Explained variance ratio: {svd_lsa.explained_variance_ratio_.sum():.2%}\")\n\n# Compute cosine similarity\nlsa_cosine_sim = cosine_similarity(lsa_word_vectors)"
  },
  {
   "cell_type": "markdown",
   "id": "neighbors-comparison",
   "metadata": {},
   "source": [
    "## Compare Nearest Neighbors Across Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nearest-neighbors",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_neighbors(word, word2idx, idx2word, sim_matrix, k=5):\n",
    "    \"\"\"Find k nearest neighbors using precomputed similarity matrix.\"\"\"\n",
    "    if word not in word2idx:\n",
    "        return []\n",
    "    \n",
    "    word_idx = word2idx[word]\n",
    "    similarities = sim_matrix[word_idx]\n",
    "    top_indices = np.argsort(similarities)[::-1][:k+1]\n",
    "    \n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        if idx != word_idx:\n",
    "            results.append((idx2word[idx], similarities[idx]))\n",
    "        if len(results) >= k:\n",
    "            break\n",
    "    return results\n",
    "\n",
    "focus_words = ['king', 'queen', 'man', 'woman', 'city', 'country', 'good', 'bad']\n",
    "focus_words = [w for w in focus_words if w in word2idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-neighbors",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all three count-based methods\n",
    "print(\"Nearest Neighbors Comparison (top 3)\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "for word in focus_words[:4]:\n",
    "    cooccur_nn = find_nearest_neighbors(word, word2idx, idx2word, cooccur_cosine_sim, k=3)\n",
    "    svd_nn = find_nearest_neighbors(word, word2idx, idx2word, svd_cooccur_cosine_sim, k=3)\n",
    "    lsa_nn = find_nearest_neighbors(word, word2idx, idx2word, lsa_cosine_sim, k=3)\n",
    "    \n",
    "    print(f\"\\n{word}:\")\n",
    "    print(f\"  Co-occurrence: {', '.join([w for w, _ in cooccur_nn])}\")\n",
    "    print(f\"  SVD(cooccur):  {', '.join([w for w, _ in svd_nn])}\")\n",
    "    print(f\"  LSA(doc-word): {', '.join([w for w, _ in lsa_nn])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "word2vec-intro",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 4: Word2Vec\n",
    "\n",
    "Prediction-based embeddings: predict context words from center word.\n",
    "\n",
    "$$P(o|c) = \\frac{\\exp(u_o^\\top v_c)}{\\sum_{w \\in V} \\exp(u_w^\\top v_c)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-word2vec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "print(\"Training Word2Vec...\")\n",
    "model = Word2Vec(\n",
    "    sentences=tokenized,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=5,\n",
    "    sg=1,  # Skip-gram\n",
    "    workers=4,\n",
    "    seed=42\n",
    ")\n",
    "word_vectors = model.wv\n",
    "print(f\"Trained word vectors for {len(word_vectors)} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "word2vec-neighbors",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec nearest neighbors\n",
    "print(\"Word2Vec Nearest Neighbors (top 5):\")\n",
    "for word in focus_words[:4]:\n",
    "    if word in word_vectors:\n",
    "        similar = word_vectors.most_similar(word, topn=5)\n",
    "        print(f\"  {word}: {', '.join([w for w, _ in similar])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analogy-intro",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 5: Analogy Evaluation\n",
    "\n",
    "Evaluate all embedding methods on the Google analogy dataset.\n",
    "\n",
    "For analogy **a : b :: c : ?**, compute $\\vec{b} - \\vec{a} + \\vec{c}$ and find nearest neighbor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-analogy-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "\n",
    "# Download Google analogy dataset\n",
    "analogy_url = \"https://raw.githubusercontent.com/nicholas-leonard/word2vec/master/questions-words.txt\"\n",
    "analogy_file = \"questions-words.txt\"\n",
    "\n",
    "if not os.path.exists(analogy_file):\n",
    "    print(\"Downloading Google analogy dataset...\")\n",
    "    urllib.request.urlretrieve(analogy_url, analogy_file)\n",
    "\n",
    "def load_analogy_dataset(filepath):\n",
    "    \"\"\"Load Google analogy dataset.\"\"\"\n",
    "    analogies = {}\n",
    "    current_category = None\n",
    "    \n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip().lower()\n",
    "            if line.startswith(':'):\n",
    "                current_category = line[2:]\n",
    "                analogies[current_category] = []\n",
    "            elif line and current_category:\n",
    "                parts = line.split()\n",
    "                if len(parts) == 4:\n",
    "                    analogies[current_category].append(parts)\n",
    "    return analogies\n",
    "\n",
    "analogy_dataset = load_analogy_dataset(analogy_file)\n",
    "print(f\"Loaded {len(analogy_dataset)} analogy categories\")\n",
    "\n",
    "# Show total questions\n",
    "total = sum(len(q) for q in analogy_dataset.values())\n",
    "print(f\"Total questions: {total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analogy-evaluation-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_analogies_matrix(vectors, word2idx, idx2word, analogy_dataset):\n",
    "    \"\"\"\n",
    "    Evaluate analogies using a word vector matrix.\n",
    "    For a:b::c:d, compute b - a + c and find nearest neighbor.\n",
    "    \"\"\"\n",
    "    # Normalize vectors for cosine similarity\n",
    "    norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "    norms[norms == 0] = 1\n",
    "    vectors_norm = vectors / norms\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for category, questions in analogy_dataset.items():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        skipped = 0\n",
    "        \n",
    "        for a, b, c, expected in questions:\n",
    "            # Check if all words are in vocabulary\n",
    "            if not all(w in word2idx for w in [a, b, c, expected]):\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            total += 1\n",
    "            \n",
    "            # Get indices\n",
    "            a_idx, b_idx, c_idx = word2idx[a], word2idx[b], word2idx[c]\n",
    "            expected_idx = word2idx[expected]\n",
    "            \n",
    "            # Compute b - a + c\n",
    "            query = vectors_norm[b_idx] - vectors_norm[a_idx] + vectors_norm[c_idx]\n",
    "            query_norm = query / (np.linalg.norm(query) + 1e-10)\n",
    "            \n",
    "            # Find most similar (excluding a, b, c)\n",
    "            similarities = vectors_norm @ query_norm\n",
    "            similarities[[a_idx, b_idx, c_idx]] = -np.inf  # exclude input words\n",
    "            \n",
    "            predicted_idx = np.argmax(similarities)\n",
    "            \n",
    "            if predicted_idx == expected_idx:\n",
    "                correct += 1\n",
    "        \n",
    "        accuracy = correct / total if total > 0 else 0\n",
    "        results[category] = {\n",
    "            'correct': correct,\n",
    "            'total': total,\n",
    "            'skipped': skipped,\n",
    "            'accuracy': accuracy\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def evaluate_analogies_gensim(wv, analogy_dataset):\n",
    "    \"\"\"Evaluate analogies using gensim KeyedVectors.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for category, questions in analogy_dataset.items():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        skipped = 0\n",
    "        \n",
    "        for a, b, c, expected in questions:\n",
    "            if not all(w in wv for w in [a, b, c, expected]):\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            total += 1\n",
    "            \n",
    "            try:\n",
    "                predicted = wv.most_similar(positive=[b, c], negative=[a], topn=1)[0][0]\n",
    "                if predicted == expected:\n",
    "                    correct += 1\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        accuracy = correct / total if total > 0 else 0\n",
    "        results[category] = {\n",
    "            'correct': correct,\n",
    "            'total': total,\n",
    "            'skipped': skipped,\n",
    "            'accuracy': accuracy\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def summarize_results(results, name):\n",
    "    \"\"\"Summarize analogy results.\"\"\"\n",
    "    total_correct = sum(r['correct'] for r in results.values())\n",
    "    total_questions = sum(r['total'] for r in results.values())\n",
    "    overall_acc = total_correct / total_questions if total_questions > 0 else 0\n",
    "    return {\n",
    "        'name': name,\n",
    "        'overall_accuracy': overall_acc,\n",
    "        'total_correct': total_correct,\n",
    "        'total_questions': total_questions,\n",
    "        'by_category': results\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-all-evaluations",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all methods\n",
    "print(\"Evaluating all embedding methods on analogy task...\")\n",
    "print(\"(This may take a few minutes)\\n\")\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "# 1. Raw co-occurrence\n",
    "print(\"1. Evaluating Co-occurrence vectors...\")\n",
    "cooccur_results = evaluate_analogies_matrix(cooccur_matrix, word2idx, idx2word, analogy_dataset)\n",
    "all_results['Co-occurrence'] = summarize_results(cooccur_results, 'Co-occurrence')\n",
    "\n",
    "# 2. SVD on co-occurrence\n",
    "print(\"2. Evaluating SVD(co-occurrence) vectors...\")\n",
    "svd_results = evaluate_analogies_matrix(svd_cooccur_vectors, word2idx, idx2word, analogy_dataset)\n",
    "all_results['SVD(cooccur)'] = summarize_results(svd_results, 'SVD(cooccur)')\n",
    "\n",
    "# 3. LSA (SVD on document-word matrix)\n",
    "print(\"3. Evaluating LSA vectors...\")\n",
    "lsa_results = evaluate_analogies_matrix(lsa_word_vectors, word2idx, idx2word, analogy_dataset)\n",
    "all_results['LSA'] = summarize_results(lsa_results, 'LSA')\n",
    "\n",
    "# 4. Word2Vec\n",
    "print(\"4. Evaluating Word2Vec vectors...\")\n",
    "w2v_results = evaluate_analogies_gensim(word_vectors, analogy_dataset)\n",
    "all_results['Word2Vec'] = summarize_results(w2v_results, 'Word2Vec')\n",
    "\n",
    "print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-overall-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display overall results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"OVERALL ANALOGY EVALUATION RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Method':<20} {'Accuracy':>12} {'Correct':>12} {'Total':>12}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for method, result in all_results.items():\n",
    "    print(f\"{method:<20} {result['overall_accuracy']:>12.1%} {result['total_correct']:>12} {result['total_questions']:>12}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed comparison by category\n",
    "categories = list(analogy_dataset.keys())\n",
    "methods = list(all_results.keys())\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"ACCURACY BY CATEGORY\")\n",
    "print(\"=\"*100)\n",
    "print(f\"{'Category':<30} \" + \" \".join([f\"{m:>15}\" for m in methods]))\n",
    "print(\"-\"*100)\n",
    "\n",
    "for cat in categories:\n",
    "    accs = []\n",
    "    for method in methods:\n",
    "        acc = all_results[method]['by_category'][cat]['accuracy']\n",
    "        accs.append(f\"{acc:>14.1%}\")\n",
    "    print(f\"{cat:<30} \" + \" \".join(accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "methods = list(all_results.keys())\n",
    "accuracies = [all_results[m]['overall_accuracy'] * 100 for m in methods]\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "\n",
    "bars = ax.bar(methods, accuracies, color=colors)\n",
    "ax.set_ylabel('Accuracy (%)')\n",
    "ax.set_title('Analogy Task: Overall Accuracy by Method')\n",
    "ax.set_ylim(0, max(accuracies) * 1.2)\n",
    "\n",
    "# Add value labels\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "            f'{acc:.1f}%', ha='center', va='bottom', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "category-comparison-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot by category (semantic vs syntactic)\n",
    "semantic_cats = ['capital-common-countries', 'capital-world', 'currency', 'city-in-state', 'family']\n",
    "syntactic_cats = [c for c in categories if c not in semantic_cats]\n",
    "\n",
    "def get_grouped_accuracy(results, cat_list):\n",
    "    correct = sum(results['by_category'][c]['correct'] for c in cat_list if c in results['by_category'])\n",
    "    total = sum(results['by_category'][c]['total'] for c in cat_list if c in results['by_category'])\n",
    "    return correct / total if total > 0 else 0\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "x = np.arange(len(methods))\n",
    "width = 0.35\n",
    "\n",
    "semantic_accs = [get_grouped_accuracy(all_results[m], semantic_cats) * 100 for m in methods]\n",
    "syntactic_accs = [get_grouped_accuracy(all_results[m], syntactic_cats) * 100 for m in methods]\n",
    "\n",
    "bars1 = ax.bar(x - width/2, semantic_accs, width, label='Semantic', color='steelblue')\n",
    "bars2 = ax.bar(x + width/2, syntactic_accs, width, label='Syntactic', color='coral')\n",
    "\n",
    "ax.set_ylabel('Accuracy (%)')\n",
    "ax.set_title('Analogy Task: Semantic vs Syntactic')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(methods)\n",
    "ax.legend()\n",
    "\n",
    "# Add value labels\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, height + 0.3,\n",
    "                f'{height:.1f}%', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": "## Summary\n\n**Count-based methods:**\n- **Co-occurrence**: raw word-context counts (high-dimensional, sparse)\n- **SVD(cooccur)**: SVD on word-context matrix (dense, lower-dimensional)\n- **LSA**: SVD on document-word matrix (captures document-level patterns)\n\n**Prediction-based methods:**\n- **Word2Vec**: predict context words, learn vectors via SGD\n\n**Key insight:** SVD and Word2Vec are closely related! ([Levy & Goldberg, 2014](https://papers.nips.cc/paper_files/paper/2014/hash/b78666971ceae55a8e87efb7cbfd9ad4-Abstract.html))"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}